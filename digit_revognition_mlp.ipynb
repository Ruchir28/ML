{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT MNIST DIGIT DATASET\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "# load validation and test data\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "## split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using transoform.ToTensor() to convert the image to a tensor\n",
    "## i.e a multi-dimensional array of numbers with a shape of (1, 28, 28)\n",
    "## The pixel values in the image are normalized to the range of 0 to 1\n",
    "## and then these values are normalized to the range of -1 to 1 using the transform.Normalize() function\n",
    "## That is done by subtracting 0.5 from each pixel value and then dividing by 0.5 to get the pixel value in the range of -1 to 1\n",
    "## assumption here is that the mean of the pixel values is 0.5 and the standard deviation is 0.5\n",
    "## so subtracting 0.5 from each pixel value will make the mean 0 and dividing by 0.5(i.e already existing sd) will make the standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "demo_image_tensor, demo_image_target = train_loader.dataset[5]\n",
    "print(\"--\",demo_image_target)\n",
    "images, labels = dataiter.__next__()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "plt.imshow(demo_image_tensor.numpy().squeeze(), cmap='gray_r')\n",
    "\n",
    "figure = plt.figure()\n",
    "num_of_images = 60\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(6, 10, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = torch.nn.Sigmoid() ## using sigmoid activation function, because when we \n",
    "        ## are normalizing the pixel values to the range of -1 to 1 , we wouldn't want to lose the negative values\n",
    "        self.fc1 = torch.nn.Linear(28*28, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "num_epochs = 15\n",
    "train_loss_history = list()\n",
    "val_loss_history = list()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward() # this is where the model learns by backpropagating\n",
    "        optimizer.step() # this is where the model optimizes its weights\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (outputs.argmax(1) == targets).sum().item() \n",
    "        train_total += targets.size()[0]\n",
    "    print(f'Epoch {epoch + 1} training accuracy: {((train_correct/train_total*1.0)*100)}% training loss: {train_loss/len(train_loader):.5f}')\n",
    "    scheduler.step()\n",
    "    train_loss_history.append(train_loss/len(train_loader))   \n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    net.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        val_correct += (preds == labels).sum().item()\n",
    "        val_total += labels.size()[0]\n",
    "        val_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1} validation accuracy: {((val_correct/val_total*1.0)*100)}% validation loss: {val_loss/len(test_loader):.5f}')\n",
    "    val_loss_history.append(val_loss/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "torch.save(net.state_dict(), 'mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
